# JAVA读取hdfs上压缩文件

### 压缩文件格式有很多种，如何构造一个通用的解压器读取压缩文件，如下

```java
package com.teu.util;

import org.apache.avro.Schema;
import org.apache.avro.file.*;
import org.apache.avro.generic.GenericDatumReader;
import org.apache.avro.generic.GenericDatumWriter;
import org.apache.avro.io.DatumWriter;
import org.apache.avro.io.EncoderFactory;
import org.apache.avro.io.JsonEncoder;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.*;
import org.apache.hadoop.io.*;
import org.apache.hadoop.io.compress.CompressionCodec;
import org.apache.hadoop.io.compress.CompressionCodecFactory;
import org.apache.hadoop.util.ReflectionUtils;
import org.codehaus.jackson.JsonEncoding;
import org.codehaus.jackson.JsonFactory;
import org.codehaus.jackson.JsonGenerator;
import org.codehaus.jackson.util.MinimalPrettyPrinter;

import java.io.*;
import java.util.zip.GZIPInputStream;

/**
 * Created by selamat on 15/11/2.
 */
public class Constants {

    public void ReaderHDFS(Path path) {
        try {
            Configuration conf = getConf();
            FileSystem fs = FileSystem.get(conf);
            if(fs.exists(path)) {
                InputStream in = getInputStream(conf, fs, path);
                String line = null;
                BufferedReader br = new BufferedReader(new InputStreamReader(in));
                while((line = br.readLine()) != null) {
                    System.out.println(line);
                }
                in.close();
            } else {
                System.err.println("file not found.");
            }
        } catch(IOException ioe) {
            ioe.printStackTrace();
        } finally {
            System.err.println("read file failed.");
        }
    }

    private Configuration getConf() {
        return new Configuration();
    }

    /**
     * 通过读取第一个字符判断需要构造的输入流
     *
     * @param conf  构造配置文件
     * @param fs    文件系统实例
     * @param path  读取文件路径
     * @return
     * @throws IOException
     */
    protected InputStream getInputStream(Configuration conf, FileSystem fs, Path path) throws IOException {
        FileStatus fileStatus = fs.getFileStatus(path);
        FSDataInputStream i = (FSDataInputStream) fs.open(path);

        // Handle 0 and 1-byte files
        short leadBytes;
        try {
            leadBytes = i.readShort();
        } catch (EOFException e) {
            i.seek(0);
            return i;
        }

        // Check type of stream first
        switch(leadBytes) {
            case 0x1f8b: { // RFC 1952
                // Must be gzip
                i.seek(0);
                return new GZIPInputStream(i);
            }
            case 0x5345: { // 'S' 'E'
                // Might be a SequenceFile
                if (i.readByte() == 'Q') {
                    i.close();
                    return new TextRecordInputStream(fileStatus);
                }
            }
            default: {
                // Check the type of compression instead, depending on Codec class's
                // own detection methods, based on the provided path.
                CompressionCodecFactory cf = new CompressionCodecFactory(conf);
                CompressionCodec codec = cf.getCodec(path);
                if (codec != null) {
                    i.seek(0);
                    return codec.createInputStream(i);
                }
                break;
            }
            case 0x4f62: { // 'O' 'b'
                if (i.readByte() == 'j') {
                    i.close();
                    return new AvroFileInputStream(fileStatus);
                }
                break;
            }
        }

        // File is non-compressed, or not a file container we know.
        i.seek(0);
        return i;
    }

    protected class TextRecordInputStream extends InputStream {
        SequenceFile.Reader r;
        WritableComparable<?> key;
        Writable val;

        DataInputBuffer inbuf;
        DataOutputBuffer outbuf;

        public TextRecordInputStream(FileStatus f) throws IOException {
            final Path fpath = f.getPath();
            final Configuration lconf = getConf();
            r = new SequenceFile.Reader(lconf,
                    SequenceFile.Reader.file(fpath));
            key = ReflectionUtils.newInstance(
                    r.getKeyClass().asSubclass(WritableComparable.class), lconf);
            val = ReflectionUtils.newInstance(
                    r.getValueClass().asSubclass(Writable.class), lconf);
            inbuf = new DataInputBuffer();
            outbuf = new DataOutputBuffer();
        }

        @Override
        public int read() throws IOException {
            int ret;
            if (null == inbuf || -1 == (ret = inbuf.read())) {
                if (!r.next(key, val)) {
                    return -1;
                }
                byte[] tmp = key.toString().getBytes();
                outbuf.write(tmp, 0, tmp.length);
                outbuf.write('\t');
                tmp = val.toString().getBytes();
                outbuf.write(tmp, 0, tmp.length);
                outbuf.write('\n');
                inbuf.reset(outbuf.getData(), outbuf.getLength());
                outbuf.reset();
                ret = inbuf.read();
            }
            return ret;
        }

        @Override
        public void close() throws IOException {
            r.close();
            super.close();
        }
    }

    /**
     * This class transforms a binary Avro data file into an InputStream
     * with data that is in a human readable JSON format.
     */
    protected static class AvroFileInputStream extends InputStream {
        private int pos;
        private byte[] buffer;
        private ByteArrayOutputStream output;
        private org.apache.avro.file.FileReader<?> fileReader;
        private DatumWriter<Object> writer;
        private JsonEncoder encoder;

        public AvroFileInputStream(FileStatus status) throws IOException {
            pos = 0;
            buffer = new byte[0];
            GenericDatumReader<Object> reader = new GenericDatumReader<Object>();
            FileContext fc = FileContext.getFileContext(new Configuration());
            fileReader =
                    DataFileReader.openReader(new AvroFSInput(fc, status.getPath()),reader);
            Schema schema = fileReader.getSchema();
            writer = new GenericDatumWriter<Object>(schema);
            output = new ByteArrayOutputStream();
            JsonGenerator generator =
                    new JsonFactory().createJsonGenerator(output, JsonEncoding.UTF8);
            MinimalPrettyPrinter prettyPrinter = new MinimalPrettyPrinter();
            prettyPrinter.setRootValueSeparator(System.getProperty("line.separator"));
            generator.setPrettyPrinter(prettyPrinter);
            encoder = EncoderFactory.get().jsonEncoder(schema, generator);
        }

        /**
         * Read a single byte from the stream.
         */
        @Override
        public int read() throws IOException {
            if (pos < buffer.length) {
                return buffer[pos++];
            }
            if (!fileReader.hasNext()) {
                return -1;
            }
            writer.write(fileReader.next(), encoder);
            encoder.flush();
            if (!fileReader.hasNext()) {
                // Write a new line after the last Avro record.
                output.write(System.getProperty("line.separator").getBytes());
                output.flush();
            }
            pos = 0;
            buffer = output.toByteArray();
            output.reset();
            return read();
        }

        /**
         * Close the stream.
         */
        @Override
        public void close() throws IOException {
            fileReader.close();
            output.close();
            super.close();
        }
    }
}
```

如果用maven管理工程，则在pom中引入

```xml
<dependency>
 <groupId>org.apache.hadoop</groupId>
 <artifactId>hadoop-common</artifactId>
 <version>2.6.0-cdh5.4.4</version>
</dependency>
<dependency>
 <groupId>org.apache.hadoop</groupId>
 <artifactId>hadoop-mapreduce-client-core</artifactId>
 <version>2.6.0-cdh5.4.4</version>
</dependency>
```
如果是普通工程，则引入hadoop/share下jar包即可


