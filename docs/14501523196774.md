# YARN LOG 副本数异常

## 1. 背景描述

yarn-site中增加下述配置之后

```xml
<property>
  <name>yarn.log-aggregation-enable</name>
  <value>true</value>
</property>

<property>
  <description>Where to aggregate logs</description>
  <name>yarn.nodemanager.remote-app-log-dir</name>
  <value>/home/yarn/apps/logs</value>
</property>
```

yarn会自动将application各个container的log上传的hdfs**/home/yarn/apps/logs**该目录下，但这个目录下文件一直报missing block

## 2. 产生原因

由于最近在下机器，一次下掉的机器<3 , 避免有block丢失，但此时仍然会有missing block，说明该目录下文件本来就是单副本。 由于该目录下都是存储mapreduce执行日志，而执行日志由**AppLogAggregatorImpl**将container日志上传，故分析问题应出现在此类中。

## 3. 解决

**AppLogAggregatorImpl** 中 run方法是这个线程的入口，run->doAppLogAggregation，doAppLogAggregation方法检测app是否完成，没有完成前周期性调用uploadLogsForContainers方法，进行增量log上传。

```java
while (!this.appFinishing.get() && !this.aborted.get()) {
 synchronized(this) {
   try {
     if (this.rollingMonitorInterval > 0) {
       wait(this.rollingMonitorInterval * 1000);
       if (this.appFinishing.get() || this.aborted.get()) {
         break;
       }
       uploadLogsForContainers();
     } else {
       wait(THREAD_SLEEP_TIME);
     }
   } catch (InterruptedException e) {
     LOG.warn("PendingContainers queue is interrupted");
     this.appFinishing.set(true);
   }
 }
}

```

uploadLogsForContainers函数中一步初始化writer，如下：

```java
LogWriter writer = null;
try {
 try {
   writer =
       new LogWriter(this.conf, this.remoteNodeTmpLogFileForApp,
         this.userUgi);
   // Write ACLs once when the writer is created.
   writer.writeApplicationACLs(appAcls);
   writer.writeApplicationOwner(this.userUgi.getShortUserName());

 } catch (IOException e1) {
   LOG.error("Cannot create writer for app " + this.applicationId
       + ". Skip log upload this time. ");
   return;
 }
```

以及函数LogWriter如下：

```java
public LogWriter(final Configuration conf, final Path remoteAppLogFile,
        UserGroupInformation userUgi) throws IOException {
 try {
   this.fsDataOStream =
       userUgi.doAs(new PrivilegedExceptionAction<FSDataOutputStream>() {
         @Override
         public FSDataOutputStream run() throws Exception {
           fc = FileContext.getFileContext(conf);
           fc.setUMask(APP_LOG_FILE_UMASK);
           return fc.create(
               remoteAppLogFile,
               EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),
               new Options.CreateOpts[] {});
         }
       });
 } catch (InterruptedException e) {
   throw new IOException(e);
 }
 
 // Keys are not sorted: null arg
 // 256KB minBlockSize : Expected log size for each container too
 this.writer =
     new TFile.Writer(this.fsDataOStream, 256 * 1024, conf.get(
         YarnConfiguration.NM_LOG_AGG_COMPRESSION_TYPE,
         YarnConfiguration.DEFAULT_NM_LOG_AGG_COMPRESSION_TYPE), null, conf);
 //Write the version string
 writeVersion();
}
```

上述**new Options.CreateOpts[] {}**中并未设置相关参数，继续往下在 **AbstractFileSystem.create** 方法中有如下代码：

```java
FsServerDefaults ssDef = getServerDefaults();
if (ssDef.getBlockSize() % ssDef.getBytesPerChecksum() != 0) {
 throw new IOException("Internal error: default blockSize is" + 
     " not a multiple of default bytesPerChecksum ");
}
    
if (blockSize == -1) {
 blockSize = ssDef.getBlockSize();
}

// Create a checksum option honoring user input as much as possible.
// If bytesPerChecksum is specified, it will override the one set in
// checksumOpt. Any missing value will be filled in using the default.
ChecksumOpt defaultOpt = new ChecksumOpt(
   ssDef.getChecksumType(),
   ssDef.getBytesPerChecksum());
checksumOpt = ChecksumOpt.processChecksumOpt(defaultOpt,
   checksumOpt, bytesPerChecksum);

if (bufferSize == -1) {
 bufferSize = ssDef.getFileBufferSize();
}
if (replication == -1) {
 replication = ssDef.getReplication();
}
```

其中的副本为1。这和设置viewfs的选项有关系，如果是hdfs的默认选项就没事了。这块可以在logWriter创建的时候增加一个Replication的createOpt，并设置3就可以解决了。在 **AggregatedLogFormat#LogWriter**构造函数中增加

```java
try {
   final short replica = 3;
   this.fsDataOStream =
       userUgi.doAs(new PrivilegedExceptionAction<FSDataOutputStream>() {
         @Override
         public FSDataOutputStream run() throws Exception {
           fc = FileContext.getFileContext(conf);
           fc.setUMask(APP_LOG_FILE_UMASK);
           return fc.create(
               remoteAppLogFile,
               EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),
//                    new Options.CreateOpts[] {});
               Options.CreateOpts.repFac(replica));
         }
       });
 } catch (InterruptedException e) {
   throw new IOException(e);
 }
```

