# Spark DataFrame初试

## 1. 业务背景

由于hive出现较早，并且与mapreduce结合较为紧密，所以大部分公司都采用hive做数据仓库，但随着spark的出现，部分业务线需要用spark提供的dataframe操作数据，数据源可能有很多种，比如hdfs上文件，本地文件，mysql数据表或hive数据表，前几种比较容易实现，本文只是给出示例，着重介绍最后一种spark sql与hive共享元数据

## 2. dataframe介绍

DataFrame让Spark具备了处理大规模结构化数据的能力，在比原有的RDD转化方式易用的前提下，计算性能更还快了两倍。DataFrame像是一条联结所有主流数据源并自动转化为可并行处理格式的水渠。

以一个常见的场景 -- 日志解析为例，有时我们需要用到一些额外的结构化数据（比如做IP和地址的映射），通常这样的数据会存在MySQL，而访问的方式有两种：一是每个worker远程去检索数据库，弊端是耗费额外的网络I/O资源；二是使用JdbcRDD的API转化为RDD格式，然后编写繁复的函数去实现检索，显然要写更多的代码。而现在，Spark提供了一种新的选择，一行代码就能实现从MySQL到DataFrame的转化，并且支持SQL查询。

* 本地文件或hdfs文件

```json
{"name":"Michael"}
{"name":"Andy", "age":30}
{"name":"Justin", "age":19}
```
进入spark-shell之后，执行

```scala
scala> val df = sqlContext.jsonFile("/path/to/your/jsonfile")
df: org.apache.spark.sql.DataFrame = [age: bigint, name: string]
```
从控制台的提示可以得知，我们成功创建了一个DataFrame的对象，包含age和name两个字段。

```scala
// 输出表结构
df.printSchema()

// 选择所有年龄大于21岁的人，只保留name字段
df.filter(df("age") > 21).select("name").show()

// 选择name，并把age字段自增
df.select("name", df("age") + 1).show()

// 按年龄分组计数
df.groupBy("age").count().show()

// 左联表（注意是3个等号！）
df.join(df2, df("name") === df2("name"), "left").show()
```

此外，我们也可以把DataFrame对象转化为一个虚拟的表，然后用SQL语句查询，比如下面的命令就等同于**df.groupBy("age").count().show()**

```scala
df.registerTempTable("people")
sqlContext.sql("select age, count(*) from people group by age").show()
```

* Mysql数据源

除了JSON之外，DataFrame现在已经能支持MySQL、Hive、HDFS、PostgreSQL等外部数据源，而对关系数据库的读取，是通过jdbc实现的。

对于不同的关系数据库，必须在SPARK_CLASSPATH变量中加入对应connector的jar包，比如希望连接MySQL的话应该这么启动spark-shell：

```shell
SPARK_CLASSPATH=mysql-connector-java-x.x.x-bin.jar spark-shell
```

接下来将Mysql表转化为DataFrame对象

```scala
val jdbcDF = sqlContext.load("jdbc", Map("url" -> "jdbc:mysql://localhost:3306/your_database?user=your_user&password=your_password", "dbtable" -> "your_table"))
```

* Hive

spark-sql 访问hive元数据只是通过hive-site.xml文件，取得hive元数据库，再通过mysql-connector连接数据库，解析hive元信息获得，所以需要在spark-env.sh中，加入如下几个jar包
	* hive-contrib.jar
	* regexp-1.3.jar
	* mysql-connector-java-5.1.26.jar (具体版本可自选，能用即可)

并将hive-site.xml拷贝到spark/conf目录下

*注：上述完成后执行show databases时，可能会出现HiveAdmin类找不到的情况，删除掉hive-site.xml中HiveAdmin相关配置即可*

执行 **./bin/spark-sql --master yarn-client** 进入spark-sql界面

```scala
scala> val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
sqlContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@6dcc664b

scala> sqlContext.sql("SELECT area_code,event_code,COUNT(udid) AS user_cnt FROM event_db.user_event WHERE create_date='2015-05-11' GROUP BY area_code,event_code LIMIT 10").collect().foreach(println)
```

## 3. hive, spark standalone, spark on yarn 三种方式查询同一份数据时间对比

运行模式 | 时间(s)
--------- | -------------
hive | 189.695
spark standalone | 82.895
spark on yarn | 104.259

*注：数据来源于网上，实际对比相差无几，并且默认spark启动executor是512M，增大内存后，效果会更佳*



