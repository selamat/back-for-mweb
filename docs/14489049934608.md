## Hadoop Distcp 异常

### 1. 拷贝命令


### 2. 异常

* **mismatch in length 异常**

```java
15/11/04 21:37:49 INFO mapreduce.Job main: Task Id : attempt_1446432176008_4221_m_000004_1, Status : FAILED
Error: java.io.IOException: File copy failed: hftp://10.9.14.196:50070/flume/dsap/rawdata/wuxian/lego/base/20150903/k426H_10.9.18.143_legobase.log.2015090310.1441249583051 --> hdfs://hdp-58-cluster/home/hdp_ubu_wuxian/history/lego/base/20150903/k426H_10.9.18.143_legobase.log.2015090310.1441249583051
        at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:284)
        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:252)
        at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying hftp://10.9.14.196:50070/flume/dsap/rawdata/wuxian/lego/base/20150903/k426H_10.9.18.143_legobase.log.2015090310.1441249583051 to hdfs://hdp-58-cluster/home/hdp_ubu_wuxian/history/lego/base/20150903/k426H_10.9.18.143_legobase.log.2015090310.1441249583051
        at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
        at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:280)
        ... 10 more
Caused by: java.io.IOException: Mismatch in length of source:hftp://10.9.14.196:50070/flume/dsap/rawdata/wuxian/lego/base/20150903/k426H_10.9.18.143_legobase.log.2015090310.1441249583051 and target:hdfs://hdp-58-cluster/home/hdp_ubu_wuxian/history/lego/base/.distcp.tmp.attempt_1446432176008_4221_m_000004_1
        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareFileLengths(RetriableFileCopyCommand.java:194)
        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:127)
        at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)
        at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
        ... 11 more

```

解决：hdfs文件异常关闭导致，flume或其他在写hdfs文件时，由于某些异常情况导致文件未被关闭，用hadoop fsck {file} -openforwrite 命令可以查看 -OPENFORWRITE 字样，但此时文件可读，并且读取内容并无问题，但用 **FileSystem.getFileStatus({path})** 返回文件长度为0，由此进入org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry方法，如下
	
```java
public void map(Text relPath, CopyListingFileStatus sourceFileStatus,
          Context context) throws IOException, InterruptedException {
    Path sourcePath = sourceFileStatus.getPath();

    if (LOG.isDebugEnabled())
      LOG.debug("DistCpMapper::map(): Received " + sourcePath + ", " + relPath);

    Path target = new Path(targetWorkPath.makeQualified(targetFS.getUri(),
                          targetFS.getWorkingDirectory()) + relPath.toString());

    EnumSet<DistCpOptions.FileAttribute> fileAttributes
            = getFileAttributeSettings(context);
    final boolean preserveRawXattrs = context.getConfiguration().getBoolean(
        DistCpConstants.CONF_LABEL_PRESERVE_RAWXATTRS, false);

    final String description = "Copying " + sourcePath + " to " + target;
    context.setStatus(description);

    LOG.info(description);

    try {
      CopyListingFileStatus sourceCurrStatus;
      FileSystem sourceFS;
      try {
        sourceFS = sourcePath.getFileSystem(conf);
        final boolean preserveXAttrs =
            fileAttributes.contains(FileAttribute.XATTR);
        sourceCurrStatus = DistCpUtils.toCopyListingFileStatus(sourceFS,
          sourceFS.getFileStatus(sourcePath),
          fileAttributes.contains(FileAttribute.ACL), 
          preserveXAttrs, preserveRawXattrs);
      } catch (FileNotFoundException e) {
        throw new IOException(new RetriableFileCopyCommand.CopyReadException(e));
      }

      FileStatus targetStatus = null;

      try {
        targetStatus = targetFS.getFileStatus(target);
      } catch (FileNotFoundException ignore) {
        if (LOG.isDebugEnabled())
          LOG.debug("Path could not be found: " + target, ignore);
      }

      if (targetStatus != null && (targetStatus.isDirectory() != sourceCurrStatus.isDirectory())) {
        throw new IOException("Can't replace " + target + ". Target is " +
            getFileType(targetStatus) + ", Source is " + getFileType(sourceCurrStatus));
      }

      if (sourceCurrStatus.isDirectory()) {
        createTargetDirsWithRetry(description, target, context);
        return;
      }

      FileAction action = checkUpdate(sourceFS, sourceCurrStatus, target);
      if (action == FileAction.SKIP) {
        LOG.info("Skipping copy of " + sourceCurrStatus.getPath()
                 + " to " + target);
        updateSkipCounters(context, sourceCurrStatus);
        context.write(null, new Text("SKIP: " + sourceCurrStatus.getPath()));
      } else {
        copyFileWithRetry(description, sourceCurrStatus, target, context,
            action, fileAttributes);
      }

      DistCpUtils.preserve(target.getFileSystem(conf), target, sourceCurrStatus,
          fileAttributes, preserveRawXattrs);
    } catch (IOException exception) {
      handleFailures(exception, sourceFileStatus, target, context);
    }
  }
```
解决方法，将未关闭文件关闭即可，可以通过修改DFSClient.recoverLease方法为pulibc，或者通过mv文件解决

* **EOF异常**

```java
2015-11-12 23:50:50,160 WARN [main] org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:hdp_ubu_wuxian (auth:SIMPLE) cause:java.io.IOException: File copy failed: hftp://10.9.14.196:50070/dsap/rawdata/wuxian_user_data/user/20150922/part-m-00295.lzo --> hdfs://hdp-58-cluster/home/hdp_ubu_wuxian/history/20150922/part-m-00295.lzo
2015-11-12 23:50:50,160 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: File copy failed: hftp://10.9.14.196:50070/dsap/rawdata/wuxian_user_data/user/20150922/part-m-00295.lzo --> hdfs://hdp-58-cluster/home/hdp_ubu_wuxian/history/20150922/part-m-00295.lzo
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:284)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:252)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:50)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: Couldn't run retriable-command: Copying hftp://10.9.14.196:50070/dsap/rawdata/wuxian_user_data/user/20150922/part-m-00295.lzo to hdfs://hdp-58-cluster/home/hdp_ubu_wuxian/history/20150922/part-m-00295.lzo
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:280)
	... 10 more
Caused by: org.apache.hadoop.tools.mapred.RetriableFileCopyCommand$CopyReadException: java.io.IOException: Got EOF but currentPos = 148439040 < filelength = 263641509
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.readBytes(RetriableFileCopyCommand.java:289)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:257)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToFile(RetriableFileCopyCommand.java:184)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:124)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:100)
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
	... 11 more
Caused by: java.io.IOException: Got EOF but currentPos = 148439040 < filelength = 263641509
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.update(ByteRangeInputStream.java:199)
	at org.apache.hadoop.hdfs.web.ByteRangeInputStream.read(ByteRangeInputStream.java:214)
	at java.io.DataInputStream.read(DataInputStream.java:100)
	at org.apache.hadoop.tools.util.ThrottledInputStream.read(ThrottledInputStream.java:80)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.readBytes(RetriableFileCopyCommand.java:284)
	... 16 more

```

原因：hftp连接超时所致，distcp在不同版本直接通过http方式通信，极端情况可能会有多个map同时访问源集群同一台机器，造成http连接超时，此时校验发现文件length不一致，导致task失败，代码解析依旧可以见上。

解决：通过 **-m** 调整同时拷贝的map数，可减少这种情况发生







