# HBase region abort

> Last week, a regionserver aborted at early morining.

### 1. Log

```shell

2016-08-29 06:23:18,777 WARN  [DataStreamer for file /home/hbase/WALs/tjtx-103-12.58os.org,60020,1466156306976/tjtx-103-12.58os.org%2C60020%2C1466156306976.default.1472420258415 block BP-128396532-10.126.81.226-1443498044997:blk_1126862920_53123231] hdfs.DFSClient: DataStreamer Exception
java.io.IOException: Broken pipe
        at sun.nio.ch.FileDispatcherImpl.write0(Native Method)
        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)
        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:93)
        at sun.nio.ch.IOUtil.write(IOUtil.java:65)
        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:487)
        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:63)
        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:159)
        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:117)
        at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
        at java.io.DataOutputStream.write(DataOutputStream.java:107)
        at org.apache.hadoop.hdfs.DFSOutputStream$Packet.writeTo(DFSOutputStream.java:327)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:655)
2016-08-29 06:23:18,777 WARN  [ResponseProcessor for block BP-128396532-10.126.81.226-1443498044997:blk_1126862920_53123231] hdfs.DFSClient: DFSOutputStream ResponseProcessor exception  for block BP-128396532-10.126.81.226-1443498044997:blk_1126862920_53123231
java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocolPB.PBHelper.vintPrefixed(PBHelper.java:2239)
        at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:176)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:879)
2016-08-29 06:23:18,778 WARN  [DataStreamer for file /home/hbase/WALs/tjtx-103-12.58os.org,60020,1466156306976/tjtx-103-12.58os.org%2C60020%2C1466156306976.default.1472420258415 block BP-128396532-10.126.81.226-1443498044997:blk_1126862920_53123231] hdfs.DFSClient: Error Recovery for block BP-128396532-10.126.81.226-1443498044997:blk_1126862920_53123231 in pipeline DatanodeInfoWithStorage[10.126.103.12:50010,DS-b7d82953-716b-4ac3-8f41-b61c96b136f6,DISK], DatanodeInfoWithStorage[10.126.96.150:50010,DS-42c39ecb-6c9d-4ffc-b636-aa1cf25ab72f,DISK]: bad datanode DatanodeInfoWithStorage[10.126.103.12:50010,DS-b7d82953-716b-4ac3-8f41-b61c96b136f6,DISK]
2016-08-29 06:23:18,777 INFO  [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher] regionserver.HRegionServer: regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher requesting flush for region dia_imc_data,f7e00000,1443515216939.30f2708e72f0402f0926c6893957ce0d. after a delay of 16506
2016-08-29 06:23:18,780 INFO  [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher] regionserver.HRegionServer: regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher requesting flush for region dia_imc_data,97c00000,1443515216939.1228d79f892b2f0944db1f72e55f4f83. after a delay of 15469
2016-08-29 06:23:18,780 INFO  [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher] regionserver.HRegionServer: regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher requesting flush for region dia_imc_data,45400000,1443515216939.7a108ad4b6ec1973fcf8c00a8f2ecb55. after a delay of 12000
2016-08-29 06:23:18,780 INFO  [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher] regionserver.HRegionServer: regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher requesting flush for region dia_imc_data,9e800000,1443515216939.d1ff2841226c2cd38f31ab008efa04c6. after a delay of 20995
2016-08-29 06:23:18,780 INFO  [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher] regionserver.HRegionServer: regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher requesting flush for region dia_imc_data,c9400000,1443515216939.af16c4d049a0d332592695215439756f. after a delay of 16925
2016-08-29 06:23:18,780 INFO  [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher] regionserver.HRegionServer: regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher requesting flush for region dia_imc_data,9ea00000,1443515216939.c8c91a3f11e11740e68ff29ea8b9240e. after a delay of 9294
2016-08-29 06:23:18,780 INFO  [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher] regionserver.HRegionServer: regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher requesting flush for region dia_imc_data,91e00000,1443515216939.94f641f0b5b285ae43e21d5cb731f0ed. after a delay of 15277
2016-08-29 06:23:18,780 INFO  [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher] regionserver.HRegionServer: regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher requesting flush for region dia_imc_data,9e600000,1443515216939.f1375f69585c545d1fedddeb4b7907f5. after a delay of 15909
2016-08-29 06:23:18,781 INFO  [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher] regionserver.HRegionServer: regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.periodicFlusher requesting flush for region dia_imc_data,5da00000,1443515216939.10d52e6932d516b955e09b25bd23c4ae. after a delay of 21984
2016-08-29 06:23:18,777 WARN  [sync.1] hdfs.DFSClient: Slow waitForAckedSeqno took 186428ms (threshold=30000ms)
2016-08-29 06:23:18,781 WARN  [sync.1] wal.FSHLog: HDFS pipeline error detected. Found 1 replicas but expecting no less than 2 replicas.  Requesting close of wal. current pipeline: [DatanodeInfoWithStorage[10.126.96.150:50010,DS-42c39ecb-6c9d-4ffc-b636-aa1cf25ab72f,DISK]]
2016-08-29 06:23:18,781 INFO  [sync.1] wal.FSHLog: Slow sync cost: 186431 ms, current pipeline: [DatanodeInfoWithStorage[10.126.96.150:50010,DS-42c39ecb-6c9d-4ffc-b636-aa1cf25ab72f,DISK]]
2016-08-29 06:23:18,788 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 186045ms
        GC pool 'ParNew' had collection(s): count=1 time=265ms
        GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=186159ms
        2016-08-29 06:23:18,809 INFO  [file] sink.FileSink: metric file roll, new name =
2016-08-29 06:23:18,822 WARN  [B.defaultRpcServer.handler=73,queue=8,port=60020] ipc.RpcServer: (responseTooSlow): {"processingtimems":186546,"call":"Multi(org.apache.hadoop.hbase.protobuf.generated.ClientProtos$MultiRequest)","client":"10.126.96.142:17673","starttimems":1472422812276,"queuetimems":0,"class":"HRegionServer","responsesize":1003,"method":"Multi"}
2016-08-29 06:23:18,822 WARN  [DataStreamer for file /home/hbase/WALs/tjtx-103-12.58os.org,60020,1466156306976/tjtx-103-12.58os.org%2C60020%2C1466156306976.default.1472420258415 block BP-128396532-10.126.81.226-1443498044997:blk_1126862920_53123231] hdfs.DFSClient: DataStreamer Exception
        org.apache.hadoop.ipc.RemoteException(java.io.IOException): BP-128396532-10.126.81.226-1443498044997:blk_1126862920_53123231 does not exist or is not under Constructionblk_1126862920_53128635
                at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkUCBlock(FSNamesystem.java:7215)
                at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateBlockForPipeline(FSNamesystem.java:7282)
                at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline(NameNodeRpcServer.java:749)
                at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.updateBlockForPipeline(AuthorizationProviderProxyClientProtocol.java:637)
                at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:932)
                at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
                at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
                at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1063)
                at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)
                at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)
                at java.security.AccessController.doPrivileged(Native Method)
                at javax.security.auth.Subject.doAs(Subject.java:415)
                at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
                at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)

                at org.apache.hadoop.ipc.Client.call(Client.java:1468)
                at org.apache.hadoop.ipc.Client.call(Client.java:1399)
                at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
                at com.sun.proxy.$Proxy19.updateBlockForPipeline(Unknown Source)
                at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:877)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:606)
                at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
                at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
                at com.sun.proxy.$Proxy20.updateBlockForPipeline(Unknown Source)
                at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
                at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
                at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
                at java.lang.reflect.Method.invoke(Method.java:606)
                at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
                at com.sun.proxy.$Proxy21.updateBlockForPipeline(Unknown Source)

2016-08-29 06:23:18,852 FATAL [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.append-pool1-t1] wal.FSHLog: Could not append. Requesting close of wal
org.apache.hadoop.ipc.RemoteException(java.io.IOException): BP-128396532-10.126.81.226-1443498044997:blk_1126862920_53123231 does not exist or is not under Constructionblk_1126862920_53128635
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkUCBlock(FSNamesystem.java:7215)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updateBlockForPipeline(FSNamesystem.java:7282)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline(NameNodeRpcServer.java:749)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.updateBlockForPipeline(AuthorizationProviderProxyClientProtocol.java:637)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolServerSideTranslatorPB.java:932)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1063)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)

        at org.apache.hadoop.ipc.Client.call(Client.java:1468)
        at org.apache.hadoop.ipc.Client.call(Client.java:1399)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
        at com.sun.proxy.$Proxy19.updateBlockForPipeline(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.updateBlockForPipeline(ClientNamenodeProtocolTranslatorPB.java:877)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
        at com.sun.proxy.$Proxy20.updateBlockForPipeline(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hbase.fs.HFileSystem$1.invoke(HFileSystem.java:279)
        at com.sun.proxy.$Proxy21.updateBlockForPipeline(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:1278)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:1016)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:560)
2016-08-29 06:23:18,852 FATAL [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.logRoller] regionserver.HRegionServer: ABORTING region server tjtx-103-12.58os.org,60020,1466156306976: IOE in log roller
java.io.IOException: cannot get log writer
        at org.apache.hadoop.hbase.wal.DefaultWALProvider.createWriter(DefaultWALProvider.java:365)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.createWriterInstance(FSHLog.java:741)
        at org.apache.hadoop.hbase.regionserver.wal.FSHLog.rollWriter(FSHLog.java:706)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:137)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.io.FileNotFoundException: Parent directory doesn't exist: /home/hbase/WALs/tjtx-103-12.58os.org,60020,1466156306976
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.verifyParentDir(FSNamesystem.java:2515)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2818)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2724)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2608)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:585)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.create(AuthorizationProviderProxyClientProtocol.java:110)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:395)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1063)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2081)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2077)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2075)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
        at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1743)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1662)
        at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1622)
        at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:442)
        at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:438)
        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
        at org.apache.hadoop.hdfs.DistributedFileSystem.createNonRecursive(DistributedFileSystem.java:438)
        at org.apache.hadoop.fs.FileSystem.createNonRecursive(FileSystem.java:1112)
        at org.apache.hadoop.fs.FileSystem.createNonRecursive(FileSystem.java:1088)
        at org.apache.hadoop.hbase.regionserver.wal.ProtobufLogWriter.init(ProtobufLogWriter.java:90)
        at org.apache.hadoop.hbase.wal.DefaultWALProvider.createWriter(DefaultWALProvider.java:361)
        ... 4 more
```

### 2. Track

日志中显示，regionserver在通过dfslient操作文件时抛出异常，导致regionserver挂掉。

```shell
2016-08-29 06:23:18,852 FATAL [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.logRoller] regionserver.HRegionServer: ABORTING region server tjtx-103-12.58os.org,60020,1466156306976: IOE in log roller
java.io.IOException: cannot get log writer
```

经追查代码发现，这个异常在regionserver进行flush操作时抛出，查看具体日志，发现当regionserver flush内存中数据之后，操作WAL log时，对应的WAL文件目录已经不存在，抛出异常导致regionserver挂掉。（flush操作抛出的任何异常都会使regionserver异常退出）

经追查代码发现，触发flush的原因在于如下日志

```shell
2016-08-29 06:23:18,852 FATAL [regionserver/tjtx-103-12.58os.org/10.126.103.12:60020.append-pool1-t1] wal.FSHLog: Could not append. Requesting close of wal
```

在WAL log 进行append操作是，如果写入hdfs失败，抛出该FATAL异常，并触发**requestLogRoll()**。引发block损坏的原因在于hdfs日志中，如下：

```shell
java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/10.126.103.12:50010 remote=/10.126.103.12:56667]
```

datanode一直在等待来自客户端的read请求，但是直至SocketTimeout，请求都没有过来，此时datanode会将该连接断开，并打印上述日志。

引发datanode等超时的原因也可以在regionserver的日志中找到，如下

```shell
2016-08-29 06:23:18,788 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 186045ms
        GC pool 'ParNew' had collection(s): count=1 time=265ms
        GC pool 'ConcurrentMarkSweep' had collection(s): count=2 time=186159ms
        2016-08-29 06:23:18,809 INFO  [file] sink.FileSink: metric file roll, new name =
```

由此可见，这次超长时间的HBase full gc，是导致datanode写block失败的主要原因，datanode接收数据时，client无反应，超时后，datanode断开连接，此时三副本block全部损坏。

**到这里只是解释了 datanode断开连接-> block损坏-> wal append失败-> roll log -> flush data -> WAL dir not exist 这条线路，但WAL dir为何不存在仍然没有解释。** 再往前翻HDFS日志发现：

```shell
2016-08-29 06:21:24,215 INFO org.apache.hadoop.hdfs.StateChange IPC Server handler 721 on 9000: *DIR* NameNode.rename: /home/hbase/WALs/tjtx-103-12.58os.org,60020,1466156306976 to /home/hbase/WALs/tjtx-103-12.58os.org,60020,1466156306976-splitting,ugi=hbase (auth:SIMPLE),ip=/10.126.81.136
2016-08-29 06:21:31,308 INFO org.apache.hadoop.hdfs.StateChange IPC Server handler 98 on 9000: *DIR* Namenode.delete: src=/home/hbase/WALs/tjtx-103-12.58os.org,60020,1466156306976-splitting, recursive=false,ugi=hbase (auth:SIMPLE),ip=/10.126.81.136

```

此时，终于明白，regionserver由于gc原因停顿之后，HMaster已经认为regionserver挂掉，开始log-splitting过程，进而将regionserver wal目录重命名后删除，此时再操作WAL dir必定会失败。

至此，regionserver abort原因找到，某次FULL GC时间过长导致。

### 3. 尝试解决

经过排查，HBase在宕机前经历了很严重、很频繁的Full GC，从下面日志可以进一步看出，这些Full GC都是在 concurrent mode failure模式下发生的，也就是虚拟机还未执行完本次GC的情况下又来了大量数据导致JVM内存不够，此时虚拟机会将所有用户线程挂起，执行长时间的Full GC！

```shell
(concurrent mode failure): 45876255K->21800674K(46137344K), 10.0625300 secs] 48792749K->21800674K(49283072K), [CMS Perm : 43274K->43274K(262144K)], 10.2083040 secs] [Times: user=12.02 sys=0.00, real=10.20 secs]
2016-04-14 21:22:43,990 WARN  [JvmPauseMonitor] util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 10055ms
GC pool 'ParNew' had collection(s): count=2 time=244ms
GC pool 'ConcurrentMarkSweep' had collection(s): count=1 time=10062ms
```

说明，在abort前时刻，hbase产生大量的对象，并且有大量对象进入年老代，但经过回收，这些对象占用的空间又被释放掉。所以需要对jvm进行一些调整

* 当前JVM参数中，jvm可使用内存容量为30G，年轻代只有1G，如果某一时刻产生对象过多，则会导致大量对象进入年老代（有些对象生命周期稍长，增加年轻代空间，可以使对象在年轻代存在时间更久），增加年轻代空间，并且增加survive区占比大小（此举也会带来副作用，增加年轻代大小，意味着young gc时间会有所增加）
* 关闭scan中block cache，避免scan过程中，大量block读取到内存中，一次scan如果进行时间较长，那么scan产生的临时对象可能会加入到年老代，scan结束后，则会被释放掉，所以增加年轻代有利于避免临时对象进入年老代。



