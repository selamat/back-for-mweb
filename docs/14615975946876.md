# hbase-mr-tools

### 1. 概述

该工具为提供mapreduce操作hbase相关示例代码，主要提供以下三种工具：

* 导出hbase表，全量或指定startKey和stopKey以某种特定的形式导出部分导出表（格式可以根据需求自定义）
* 已知rowkey列表（比如infoid列表），将已知rowkey列表对应的hbase表数据以某种格式导出（可以自定义）
* bulkload程序，将已知一批数据装入hbase中，可以自定义已知数据解析规范

### 2. 程序说明

* TableScanner
	* TableScanner为扫描表mr任务示例，用户可继承**com.bj58.dsap.scan
.KeyValueConstructor**类，实现其中的抽象方法实现自定义拼接需要的数据，程序包中已经提供了*IMCKVConstructor*类示例，用户可参考该类编写代码，该类中主要实现的方法为**parse**方法，传入参数为KeyValue属组对象，程序中通过遍历KeyValue数组对象进行内容拼接，并赋值
	* 启动命令（大括号中为选择参数）
		
		```shell
		${HADOOP_HOME}/bin/hadoop jar hbase-mr-tools-1.0-SNAPSHOT.jar \
		     org.apache.hadoop.hbase.mapreduce.TableScanner  \
         -Dhbase.scan.table.name=${TABLENAME} \
         -Dhbase.scan.output.dir=${OUTPUTDIR} \
         -Dhbase.scan.constructor.name=com.bj58.dsap.scan.impl.IMCKVContrustor \
         {-Dhbase.scan.ignore.null.value=false    #是否忽略空行
         -Dhbase.scan.isoutput.errorrow=true     #是否输出解析错误行
         -Dhbase.scan.column=attr:infoid,attr:cateid  \  #需要扫描的列族和列，不填默认扫描全表，必须按此格式，不能单独填写列族
         -Dhbase.scan.start.row=${startRow} #指定startRow
         -Dhbase.scan.end.row=${stopRow} #指定endRow }
		```

* RandomReader
	* RandomReader为已知rowkey列表（下面以imc数据为示例），需要从某张表（imc表）取出数据，并按照一定规则拼接
	* 以imc数据为例，用户继承**com.bj58.dsap.randomreader.KVRandomWriter**，并实现parse方法，通过遍历parse方法传入的KeyValue数组，并按照一定规则进行内容拼接，最终赋值（可参考）**com.bj58.dsap.randomreader.imp.IMCKVRandomWriter**类
	* 启动命令（大括号中为可选参数）
		
		```shell
		${HADOOP_HOME}/bin/hadoop jar hbase-mr-tools-1.0-SNAPSHOT.jar \
		     org.apache.hadoop.hbase.mapreduce.RandomReader  \
    
    	   -Dhbase.random.writer.name=com.bj58.dsap.randomreader.impl.IMCKVRandomWriter	\
         -Dhbase.random.table=${TABLENAME}               \
         -Dhbase.random.input.path=${INPUTPATH}          \
         -Dhbase.random.output.path=${OUTPUTPATH}        \
    -Dmapreduce.job.outputformat.class=org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat			\
    -Dmapreduce.output.fileoutputformat.compress.codec=com.hadoop.compression.lzo.LzoCodec			\
         -Dhbase.random.user=hdp_teu_dpd       #有些部门的private信息需要写入hdfs，故需要提供hadoop账号          \ 
         -Dhbase.random.writer.name=com.bj58.dsap.randomreader.impl.IMCKVRandomWriter    \
         -Dhbase.random.column=attr,content   # 通过rowkey需要去除哪些列族	\
    {-Dhbase.random.ignore.null.value=false    # 是否不输出null value，默认false，即为遇到null value或non-value输出空字符串  }

		```
		
* DataImportKV
	* 已知一批数据需要导入hbase中，可以通过**DataImportKV**进行导入
	* 以imc数据为例，用户通过继承**com.bj58.dsap.bulkload.KeyValueParser**类，实现parse方法即可实现自定义解析数据装入hbase，具体可参见**com.bj58.dsap.bulkload.imp.IMCParser**类
	* 启动命令
		
		```shell
		${HADOOP_HOME}/bin/hadoop jar hbase-mr-tools-1.0-SNAPSHOT.jar \
		     org.apache.hadoop.hbase.mapreduce.DataImportKV  \
         -Dhbase.dataimport.bulkimport=true \
         -Dhbase.dataimport.record.parser.name=com.bj58.dsap.bulkload.impl.IMCParser \
         -Dhbase.dataimport.input.dir=${inputDir} #数据源目录 \
         -Dhbase.dataimport.output.dir=${outputDir} #数据输出目录，在HDFS集群中应为/home/hbase_bulkimport/{hadoop账户}    \
         -Dhbase.dataimport.table.name=${表名}
		```

